{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://press.un.org/en']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingyi/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://press.un.org/en/2023/sgsm21980.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21978.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21947.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1874.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21952.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21876.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21852.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21806.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1848.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21765.doc.htm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "#from urllib.request import Request\n",
    "\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "\n",
    "urls = [seed_url]    #queue of urls to crawl\n",
    "seen = [seed_url]    #stack of urls seen so far\n",
    "url_crisis = []      #urls that contain word 'crisis'\n",
    "\n",
    "minNumUrl = 10       #minimun number of urls contain 'crisis'\n",
    "print(\"Starting with url=\"+str(urls))\n",
    "while len(urls) > 0 and len(url_crisis) < minNumUrl:\n",
    "    # DEQUEUE A URL FROM urls AND TRY TO OPEN AND READ IT\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue    #skip code below\n",
    "\n",
    "    # check if url contains the word 'crisis' and write down the content\n",
    "    pr = soup.find('a', href = \"/en/press-release\", hreflang = \"en\", text = \"Press Release\")\n",
    "    if pr:\n",
    "        content = soup.text\n",
    "        if 'crisis' in content.lower():\n",
    "            url_crisis.append(curr_url)\n",
    "            ori_html = soup.prettify(formatter='html')\n",
    "            writing_file = open(f'1_{len(url_crisis)}.txt', 'w')\n",
    "            writing_file.write(ori_html)\n",
    "            writing_file.close()\n",
    "            \n",
    "    # Put child URLs into the stack\n",
    "    for tag in soup.find_all('a', href = True): #find all tags with links\n",
    "        childUrl = tag['href'] #extract just the link\n",
    "        o_childurl = childUrl\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        if seed_url in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "for url in url_crisis:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://www.europarl.europa.eu/news/en/press-room/page/0']\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06130/parliament-argues-for-a-top-up-to-multi-annual-budget-for-crisis-response\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04923/reduce-demand-and-protect-people-in-prostitution-say-meps\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04918/svietlana-tsikhanouskaya-to-meps-support-belarusians-european-aspirations\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230911IPR04908/meps-vote-to-strengthen-eu-defence-industry-through-common-procurement\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02427/covid-19-parliament-adopts-roadmap-to-better-prepare-for-future-health-crises\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02418/semiconductors-meps-adopt-legislation-to-boost-eu-chips-industry\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230706IPR02317/ep-today\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230706IPR02316/ep-today\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "#from urllib.request import Request\n",
    "\n",
    "page = 0\n",
    "root = 'https://www.europarl.europa.eu/news/en/press-room'\n",
    "seed_url = f\"{root}/page/{page}\"\n",
    "seed_urls = [seed_url]\n",
    "urls = [seed_url]    #queue of urls to crawl\n",
    "url_ps_crisis = []      #urls that contain word 'crisis'\n",
    "\n",
    "minNumUrl = 10       #minimun number of urls contain 'crisis'\n",
    "print(\"Starting with url=\"+str(urls))\n",
    "while len(url_ps_crisis) < minNumUrl:\n",
    "    if len(urls) == 0:\n",
    "        page+=1\n",
    "        urls.append(f\"https://www.europarl.europa.eu/news/en/press-room/page/{page}\")\n",
    "        seed_urls.append(f\"https://www.europarl.europa.eu/news/en/press-room/page/{page}\")\n",
    "    try:\n",
    "        curr_url=urls[0]\n",
    "        urls=urls[1:]\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue    #skip code below\n",
    "    if curr_url not in seed_urls:\n",
    "        # check if url contains the word 'crisis' and write down the content\n",
    "        ps = soup.find('span', class_ = \"ep_name\", text = \"Plenary session\")\n",
    "        pr = soup.find('span', class_ = \"ep_name\", text = \"Press Releases\")\n",
    "        if pr and ps:\n",
    "            content = soup.text\n",
    "            if 'crisis' in content.lower():\n",
    "                url_ps_crisis.append(curr_url)\n",
    "                ori_html = soup.prettify(formatter='html')\n",
    "                writing_file = open(f'2_{len(url_ps_crisis)}.txt', 'w')\n",
    "                writing_file.write(ori_html)\n",
    "                writing_file.close()\n",
    "    else:\n",
    "        for tag in soup.find_all('a', href = True): #find all tags with links\n",
    "            childUrl = tag['href'] #extract just the link\n",
    "            childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "            if root in childUrl:\n",
    "                urls.append(childUrl)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "\n",
    "for url in url_ps_crisis:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the screenshot and txt files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
